# Task 3: Customer Churn Prediction (Bank Customers)

## ğŸ¯ Objective
Predict whether a bank customer is likely to **churn** (i.e., leave the bank) using supervised classification techniques.

## ğŸ“ Dataset
- **Name**: Churn Modelling Dataset  
- **Source**: Available on Kaggle  
- **Target Variable**: `Exited` (1 = customer left the bank, 0 = customer stayed)

## ğŸ› ï¸ Libraries Used
- `pandas` and `numpy` â€“ for data manipulation  
- `matplotlib.pyplot` and `seaborn` â€“ for visualization  
- `scikit-learn` â€“ for preprocessing, model training, and evaluation  

## ğŸ” Steps Performed

### 1. Load and Inspect Dataset
- Read the dataset using `pd.read_csv()`
- Displayed shape, columns, and initial rows to understand the structure

### 2. Data Cleaning & Preprocessing
- Removed irrelevant columns: `RowNumber`, `CustomerId`, and `Surname`
- Encoded:
  - `Gender` using **Label Encoding**
  - `Geography` using **One-Hot Encoding** (drop_first=True)
- Scaled numerical features using `StandardScaler`

### 3. Feature and Target Selection
- **Features (X)**: All variables except `Exited`  
- **Target (y)**: `Exited`

### 4. Data Splitting
- Split the data into training and test sets using `train_test_split`  
- 80% training, 20% testing

### 5. Model Training
- Trained a **Random Forest Classifier** (`RandomForestClassifier` from sklearn)

### 6. Model Evaluation
- Used `accuracy_score` to measure performance
- Created a **confusion matrix** and a **classification report** to assess precision, recall, and F1-score

### 7. Feature Importance Analysis
- Extracted feature importance from the Random Forest model
- Visualized top contributing features using a horizontal bar chart

## ğŸ“Š Results

| Metric       | Value   |
|--------------|---------|
| Accuracy     | ~85%    |
| Top Features | CreditScore, Age, Balance, Geography_Germany, NumOfProducts |

## ğŸ“Œ Key Skills Demonstrated
- âœ… Label and One-Hot Encoding  
- âœ… Binary classification modeling  
- âœ… Feature scaling with StandardScaler  
- âœ… Model evaluation using accuracy and confusion matrix  
- âœ… Feature importance interpretation

## ğŸ§  Possible Improvements
- Experiment with other models (e.g., XGBoost, Logistic Regression)
- Perform cross-validation
- Hyperparameter tuning for better accuracy
- Handle class imbalance if present (e.g., SMOTE, class weights)
